
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>RT-2</title>

    <meta name="description" content="RT-2">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://robotics-transformer2.github.io/"/>
    <meta property="og:title" content="RT-2" />
    <meta property="og:description" content="Project page for RT-2" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="RT-2" />
    <meta name="twitter:description" content="Project page for RT-2" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->


    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    
    


    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>


<nav class="navbar navbar-expand-lg navbar-light bg-white">
  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>
  <div class="collapse navbar-collapse" id="navbarNavDropdown">
    <ul class="nav justify-content-center">
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdownMenuLink" data-toggle="dropdown" aria-haspopup="true" aria-expanded="true">
          More Research
        </a>
        <div class="dropdown-menu" aria-labelledby="navbarDropdownMenuLink">
          <a class="dropdown-item" href="#">RT-1</a>
        </div>
      </li>
    </ul>
  </div>
</nav>




    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+6">RT-2: Vision-Language-Action Models</font></strong> </br> Transfer Web Knowledge to Robotic Control</br> 
                <!--<small>
                    CoRL 2021
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li>Montse Gonzalez Arenas</li> <li>Anthony Brohan</li> <li>Noah Brown</li> <li>Justice Carbajal</li> <li>Yevgen Chebotar</li> <li>Xi Chen</li> <li>Krzysztof Choromanski</li>
                 <br> <li>Tianli Ding</li> <li>Danny Driess</li> <li>Avinava Dubey</li> <li>Chelsea Finn</li> <li>Pete Florence</li> <li>Chuyuan Fu</li> <li>Keerthana Gopalakrishnan</li>
                 <br> <li>Kehang Han</li> <li>Karol Hausman</li> <li>Alex Herzog</li> <li>Jasmine Hsu</li> <li>Brian Ichter</li> <li>Alex Irpan</li> <li>Nikhil Joshi</li> <li>Ryan Julian</li> 
                 <br> <li>Dmitry Kalashnikov</li> <li>Yuheng Kuang</li> <li>Isabel Leal </li> <li>Lisa Lee</li> <li>Tsang-Wei Edward Lee</li> <li>Sergey Levine</li> <li>Yao Lu</li> <li>Henryk Michalewski</li> 
                 <br> <li>Igor Mordatch</li> <li>Karl Pertsch</li> <li>Kanishka Rao</li> <li>Krista Reymann</li> <li>Michael Ryoo</li> <li>Grecia Salazar</li> <li>Pannag Sanketi</li> <li>Pierre Sermanet</li>
                 <br>  <li>Jaspiar Singh</li> <li>Anikait Singh</li> <li>Radu Soricut</li> <li>Huong Tran</li> <li>Vincent Vanhoucke</li> <li>Quan Vuong</li> <li>Ayzaan Wahid</li> <li>Stefan Welker</li>
                 <br> <li>Paul Wohlhart</li>  <li>Jialin Wu</li> <li>Fei Xia</li> <li>Ted Xiao</li> <li>Peng Xu</li> <li>Sichun Xu</li> <li>Tianhe Yu</li> <li>Brianna Zitkovich</li> <br>
                <br>
			<b><i> Authors listed in alphabetical order (see paper appendix for contribution statement). </i></b>
		<br><br>
                    <a href="http://g.co/robotics">
                    <image src="img/deepmind.png" height="67px"> </a>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="assets/rt2.pdf">
                            <image src="img/paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>

                        <!-- <li>
                            <a href="https://youtu.be/UuKAp9a6wMs">
                            <image src="img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li> -->
                        <li>
                            <a href="http://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html">
                            <image src="img/deepmind.svg" height="60px">
                                <h4><strong>Blogpost</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

           
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <video id="v0" width="100%" playsinline autoplay muted loop>
                    <source src="videos/rt2simple.mp4" type="video/mp4">
                </video>    

                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                  We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is too sleepy (an energy drink).
                </p>       
             
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            <h3>
                Approach Overview
            </h3>
            <p style="text-align:center;">
                <img src="img/fig1.png" class="img-responsive">
            </p>

            <p class="text-justify">
               To make RT-2 easily compatible with large, pre-trained vision-language models, our recipe is simple: we represent robot actions as another language, which can be cast into text tokens and trained together with Internet-scale vision-language datasets. In particular, we co-fine-tune (a combination of fine-tuning and co-training where we keep some of the old vision & text data around) an existing vision-language model with robot data. The robot data includes the current image, language command and the robot action at the particular time step. We represent the robot actions as a following text string:  

            </p>    
            
            <p style="text-align:center;">
                <img src="img/fig3.png" class="img-responsive">
            </p>
            
            <p class="text-justify">
               Since actions are represented as text strings, one can think of them as another language that allows us to operate the robot. This simple representation makes it straightforward to fine-tune any existing vision-language model and turn it into a vision-language-action model <br><br>
               During inference, the text tokens are de-tokenized into robot actions, enabling closed loop control.This allows us to leverage the backbone and pretraining of vision-language models in learning robotic policies, transferring some of their generalization, semantic understanding, and reasoning to robotic control.  
            </p>   
            
            <p style="text-align:center;">
                <img src="img/fig2.png" class="img-responsive">
            </p>
        </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            <h3>
                Results
            </h3>
            <p style="text-align:center;">
                <img src="img/mosaic.png" class="img-responsive">
            </p>

            <p class="text-justify">
               Something about emergent properties 
            </p>    
            
            <p style="text-align:center;">
                <img src="img/rt2_emergent.png" class="img-responsive">
            </p>
            
            <p class="text-justify">
               Here we talk about generalization evals
            </p>   
            
            <p style="text-align:center;">
                <img src="img/generalization_eval.png" class="img-responsive">
            </p>
            
             <p class="text-justify">
               Here we present the generalization eval results
            </p>   
            
            <p style="text-align:center;">
                <img src="img/rt2_emergent.png" class="img-responsive">
            </p>
            
             <p class="text-justify">
               We do an ablation
            </p>   
            
            <p style="text-align:center;">
                <img src="img/ablations.png" class="img-responsive">
            </p>
            
           <p class="text-justify">
               To make results reproducible we also train it on langtable
            </p>   
            
            <p style="text-align:center;">
                <img src="img/langtable.png" class="img-responsive">
            </p>
            
            <p class="text-justify">
               Lastly CoT
            </p>   
            
            <p style="text-align:center;">
                <img src="img/CoT.png" class="img-responsive">
            </p>
        </div>
        </div>





        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
                    Videos
                </h3>
               
		<p class="text-justify">		
		Below, we show a few videos showing examples of RT-2 execution. We show that RT-2 is able to generalize to new objects, new environments, and new tasks. RT-2 is able to generalize to a variety of real-world situations that require reasoning, symbol understanding, and human recognition. 
		</p>
        <p style="test-align:center;">
            <video id="v0" width="100%" playsinline muted loop controls autoplay>
               <source src="videos/rt2_videos_compressed.mp4" type="video/mp4">
           </video>		
            </p>
        
        RT-2 can exhibit signs of chain-of-thought reasoning similarly to vision-language models. We qualitatively observe that RT-2 with chain-of-thought reasoning is able to answer more sophisticated commands due to the fact that it is given a place to plan its actions in natural language first. 
        This is a promising direction that provides some initial evidence that using LLMs or VLMs as planners can be combined with low-level policies in a single VLA model.
		<p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls autoplay>
                       <source src="videos/rt2cot_comp.mp4" type="video/mp4">
                   </video>		
        </p>

        Finally, we show that RT-2 can work on another embodiment, Language Table environment. 
        We show that RT-2 can handle real-world out-of-distribution behaviors in the Language Table environment. 

        <div id="carouselExampleCaptions3" class="carousel slide carousel-dark carousel-fade" data-bs-ride="carousel">
            <div class="carousel-indicators">
              <button type="button" data-bs-target="#carouselExampleCaptions3" data-bs-slide-to="0" class="active" aria-current="true" aria-label="Slide 1"></button>
              <button type="button" data-bs-target="#carouselExampleCaptions3" data-bs-slide-to="1" aria-label="Slide 2"></button>
              <button type="button" data-bs-target="#carouselExampleCaptions3" data-bs-slide-to="2" aria-label="Slide 2"></button>
              <button type="button" data-bs-target="#carouselExampleCaptions3" data-bs-slide-to="3" aria-label="Slide 2"></button>
              <button type="button" data-bs-target="#carouselExampleCaptions3" data-bs-slide-to="4" aria-label="Slide 2"></button>
              <button type="button" data-bs-target="#carouselExampleCaptions3" data-bs-slide-to="5" aria-label="Slide 2"></button>

            </div>
            <div class="carousel-inner">
              <div class="carousel-item active" data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                    <source src="videos/langtable/successes/01_ketchup_mustard.mp4" type="video/mp4">
                </video>	
                <div class="carousel-caption d-none d-md-block">
                  <!-- <p>Some representative placeholder content for the second slide.</p> -->
                </div>
              </div>
              <div class="carousel-item" data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                    <source src="videos/langtable/successes/02_tabasco.mp4" type="video/mp4">
                </video>	
                <div class="carousel-caption d-none d-md-block">
                  <!-- <p>Some representative placeholder content for the second slide.</p> -->
                </div>
              </div>
              <div class="carousel-item" data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                    <source src="videos/langtable/successes/03_ketchup_blue_swap.mp4" type="video/mp4">
                </video>	
                <div class="carousel-caption d-none d-md-block">
                  <!-- <p>Some representative placeholder content for the second slide.</p> -->
                </div>
              </div>
              <div class="carousel-item" data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                    <source src="videos/langtable/successes/04_red_controller.mp4" type="video/mp4">
                </video>	
                <div class="carousel-caption d-none d-md-block">
                  <!-- <p>Some representative placeholder content for the second slide.</p> -->
                </div>
              </div>
              <div class="carousel-item" data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                    <source src="videos/langtable/successes/05_white_banana.mp4" type="video/mp4">
                </video>	
                <div class="carousel-caption d-none d-md-block">
                  <!-- <p>Some representative placeholder content for the second slide.</p> -->
                </div>
              </div>
              <div class="carousel-item" data-bs-interval="100000000000">
                <video id="v0" width="100%" playsinline="" muted="" loop="" controls="" autoplay="">
                    <source src="videos/langtable/fails/fail_01_marker.mp4" type="video/mp4">
                </video>	
                <div class="carousel-caption d-none d-md-block">
                  <!-- <p>Some representative placeholder content for the second slide.</p> -->
                </div>
              </div>
            </div>
            <button class="carousel-control-prev" type="button" style="max-height:21%; margin-top:21%;" data-bs-target="#carouselExampleCaptions3" data-bs-slide="prev">
              <span class="carousel-control-prev-icon" aria-hidden="true"></span>
              <span class="visually-hidden">Previous</span>
            </button>
            <button class="carousel-control-next" type="button" style="max-height:21%; margin-top:21%;" data-bs-target="#carouselExampleCaptions3" data-bs-slide="next">
              <span class="carousel-control-next-icon" aria-hidden="true"></span>
              <span class="visually-hidden">Next</span>
            </button>
          </div>

        </div>


    
    <script>
    const myCarousel3 = document.getElementById('carouselExampleCaptions3')
    myCarousel3.addEventListener('slide.bs.carousel', event => {
        myCarousel3.getElementsByClassName("carousel-item")[event.from].getElementsByTagName("video")[0].pause();
        myCarousel3.getElementsByClassName("carousel-item")[event.to].getElementsByTagName("video")[0].play();
    })
    </script>
    



         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{rt22023arxiv,
    title={RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control},
    author={Montse Gonzalez Arenas and Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Xi Chen and Krzysztof Choromanski and Tianli Ding and Danny Driess and Avinava Dubey and Chelsea Finn and Pete Florence and Chuyuan Fu and Keerthana Gopalakrishnan and Kehang Han and Karol Hausman and Alex Herzog and Jasmine Hsu and Brian Ichter and Alex Irpan and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal  and Lisa Lee and Tsang-Wei Edward Lee and Sergey Levine and Yao Lu and Henryk Michalewski and Igor Mordatch and Karl Pertsch and Kanishka Rao and Krista Reymann and Michael Ryoo and Grecia Salazar and Pannag Sanketi and Pierre Sermanet and Jaspiar Singh and Anikait Singh and Radu Soricut and Huong Tran and Vincent Vanhoucke and Quan Vuong and Ayzaan Wahid and Stefan Welker and Paul Wohlhart and  Jialin Wu and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich},
    booktitle={TODO},
    year={2023}
}</textarea>
                </div>
            </div>
             
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
The authors would like to acknowledge Jodi Lynn Andres, Carolina Parada, Joseph Dabis, Rochelle Dela Cruz, Jessica Gomez, Gavin Gonzalez, Tomas Jackson, Jie Tan, Scott Lehrer, Dee M, Utsav Malla, Sarah Nguyen, Emily Perez, Elio Prado, Jornell Quiambao, Clayton Tan, Jodexty Therlonge, Wenxuan Zhou, and the greater Google DeepMind team for their feedback and contributions.
                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>





</div>
</div>        
</body>
</html>
